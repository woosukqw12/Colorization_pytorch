{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import cv2\n",
    "import time\n",
    "import skimage\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as vision_models\n",
    "\n",
    "# pip install fastai==2.4\n",
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_colab = None\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab1.npy', 'ab2.npy', 'ab3.npy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home = \"./input\"\n",
    "os.listdir(\"./input/ab/ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(home, channels_first=True, train_percent=0.8):\n",
    "    ab1 = np.load(os.path.join(home,\"ab/ab\", \"ab1.npy\"))\n",
    "    ab2 = np.load(os.path.join(home, \"ab/ab\", \"ab2.npy\"))\n",
    "    ab3 = np.load(os.path.join(home,\"ab/ab\", \"ab3.npy\"))\n",
    "    ab = np.concatenate([ab1, ab2, ab3], axis=0)\n",
    "#     ab = np.transpose(ab, [0, 3, 1, 2])\n",
    "    l = np.load(os.path.join(home,\"l/gray_scale.npy\"))\n",
    "    \n",
    "    \n",
    "    return train_test_split(ab,l, train_size=train_percent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ab_train, ab_test, l_train, l_test \u001b[39m=\u001b[39m load_data(home, channels_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "ab_train, ab_test, l_train, l_test = load_data(home, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(img_batch, figsize=(8,3), cmap=None):\n",
    "    if len(img_batch.shape)==3:\n",
    "        img_batch = np.expand_dims(img_batch, axis=0)\n",
    "    for img in img_batch:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(img[:,:,0].T, cmap=cmap)\n",
    "        plt.title(f\"ab-0\")\n",
    "        \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(img[:,:,1].T, cmap=cmap)\n",
    "        plt.title(f\"ab-1\")\n",
    "        \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(img[:,:,2].T, cmap=cmap)\n",
    "        plt.title(f\"l\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def plot_image(img_batch, figsize=(8,3), cmap=None, title=None):\n",
    "    if len(img_batch.shape)==3:\n",
    "        img_batch = np.expand_dims(img_batch, axis=0)\n",
    "    N = len(img_batch)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(N):\n",
    "        img = img_batch[i]\n",
    "#         img = np.transpose(img, [1,0,2])\n",
    "        plt.subplot(1,N,i+1)\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(f\"{title}\")\n",
    "    plt.show()\n",
    "        \n",
    "def to_lab(l, ab, channels_first=True):\n",
    "    if channels_first:\n",
    "        if len(l.shape)==3:\n",
    "            l = np.expand_dims(l, axis=1)\n",
    "        lab = np.concatenate([l, ab], axis=1)\n",
    "    else:\n",
    "        if len(l.shape)==3:\n",
    "            l = np.expand_dims(l, axis=3)\n",
    "        lab = np.concatenate([l, ab], axis=3)\n",
    "    return lab\n",
    "\n",
    "def lab2rgb(lab):\n",
    "    if len(lab.shape)==4:\n",
    "        arr = []\n",
    "        for img in lab:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "            arr.append(img)\n",
    "        arr = np.array(arr)\n",
    "    else:\n",
    "        arr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "    return arr\n",
    "\n",
    "def rgb2lab(rgb):\n",
    "    if len(rgb.shape)==4:\n",
    "        arr = []\n",
    "        for img in rgb:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            arr.append(img)\n",
    "        arr = np.array(arr)\n",
    "    else:\n",
    "        arr = cv2.cvtColor(rgb, cv2.COLOR_LAB2RGB)\n",
    "    return arr\n",
    "\n",
    "def to_channel_first(arr):\n",
    "    if len(arr.shape)==4:\n",
    "        arr = np.transpose(arr, [0,3,2,1])\n",
    "    else:\n",
    "        arr = np.transpose(arr, [2,1,0])\n",
    "    return arr\n",
    "\n",
    "def to_channel_last(arr):\n",
    "    if len(arr.shape)==4:\n",
    "        arr = np.transpose(arr, [0,3,2,1])\n",
    "    else:\n",
    "        arr = np.transpose(arr, [2,1,0])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout: up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
    "            out_filters //= 2\n",
    "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_expand_dim(axis):\n",
    "    def fn(arr):\n",
    "        arr = np.expand_dims(arr, axis=axis)\n",
    "        return arr\n",
    "    return fn\n",
    "\n",
    "def transform_multiply(mul):\n",
    "    def fn(arr):\n",
    "        arr = arr * mul\n",
    "        return arr\n",
    "    return fn\n",
    "\n",
    "def transform_divide(div):\n",
    "    def fn(arr):\n",
    "        arr = arr / div\n",
    "        return arr\n",
    "    return fn\n",
    "\n",
    "def model_parameters_count(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "def build_fastai_model(in_channels=1, out_channels=2, image_shape=(224, 224)):\n",
    "    model_body = create_body(resnet18(), n_in=in_channels, cut=-2)\n",
    "    model = DynamicUnet(encoder=model_body, n_out=out_channels, img_size=image_shape)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetImg(Dataset):\n",
    "    def __init__(self, l, ab, input_transforms=[], output_transforms=[]):\n",
    "        self.l = l\n",
    "        self.ab = ab\n",
    "        self.input_transforms = input_transforms\n",
    "        self.output_transforms = output_transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.l)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.l[idx]\n",
    "        y = self.ab[idx]\n",
    "        \n",
    "        if self.input_transforms is not None:\n",
    "            for fn in self.input_transforms:\n",
    "                x = fn(x)\n",
    "        \n",
    "        if self.output_transforms is not None:\n",
    "            for fn in self.output_transforms:\n",
    "                y = fn(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_channel_first(arr):\n",
    "    if len(arr.shape)==4:\n",
    "        arr = np.transpose(arr, [0,3,2,1])\n",
    "    else:\n",
    "        arr = np.transpose(arr, [2,1,0])\n",
    "    return arr\n",
    "\n",
    "def transform_expand_dim(axis):\n",
    "    def fn(arr):\n",
    "        arr = np.expand_dims(arr, axis=axis)\n",
    "        return arr\n",
    "    return fn\n",
    "\n",
    "def transform_divide(div):\n",
    "    def fn(arr):\n",
    "        arr = arr / div\n",
    "        return arr\n",
    "    return fn\n",
    "\n",
    "\n",
    "# ab_train.shape, l_train.shape, ab_test.shape, l_test.shape\n",
    "\n",
    "input_shape = [224, 224]\n",
    "batch_size = 1\n",
    "num_examples = -1\n",
    "device=  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "epochs = 1\n",
    "plot_freq = 1\n",
    "\n",
    "\n",
    "class DatasetImg(Dataset):\n",
    "    def __init__(self, l, ab, input_transforms=[], output_transforms=[]):\n",
    "        \n",
    "        self.l = l\n",
    "        self.ab = ab\n",
    "        self.input_transforms = input_transforms\n",
    "        self.output_transforms = output_transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.l)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        l = self.l[idx]\n",
    "        ab = self.ab[idx]\n",
    "        \n",
    "        if self.input_transforms is not None:\n",
    "            for fn in self.input_transforms:\n",
    "                l = fn(l)\n",
    "        \n",
    "        if self.output_transforms is not None:\n",
    "            for fn in self.output_transforms:\n",
    "                ab = fn(ab)\n",
    "        return {'L': l, 'ab': ab}\n",
    "        return l, ab\n",
    "    \n",
    "def load_data(home, channels_first=True, train_percent=0.8):\n",
    "    ab1 = np.load(os.path.join(home,\"ab/ab\", \"ab1.npy\"))\n",
    "    ab2 = np.load(os.path.join(home, \"ab/ab\", \"ab2.npy\"))\n",
    "    ab3 = np.load(os.path.join(home,\"ab/ab\", \"ab3.npy\"))\n",
    "    ab = np.concatenate([ab1, ab2, ab3], axis=0).astype(\"float32\")\n",
    "    # ab = np.transpose(ab, [0, 3, 1, 2])\n",
    "    l = np.load(os.path.join(home,\"l/gray_scale.npy\")).astype(\"float32\")\n",
    "\n",
    "\n",
    "    return train_test_split(ab,l, train_size=train_percent)\n",
    "\n",
    "ab_train, ab_test, l_train, l_test = load_data(\"input\", channels_first=True)\n",
    "    \n",
    "input_transforms = [transform_expand_dim(axis=2),\n",
    "                    to_channel_first,\n",
    "                   transform_divide(255.0)\n",
    "                   ]\n",
    "output_transforms = [\n",
    "                    to_channel_first,\n",
    "                   transform_divide(255.0)\n",
    "                   ]\n",
    "\n",
    "ds_train = DatasetImg(l_train[:num_examples], ab_train[:num_examples], input_transforms=input_transforms, output_transforms=output_transforms)\n",
    "train_dl = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ds_test = DatasetImg(l_test[:num_examples], ab_test[:num_examples], input_transforms=input_transforms, output_transforms=output_transforms)\n",
    "test_dl = DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "    \n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    \n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "    \n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "        \n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19999/19999 [35:31<00:00,  9.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "L1 Loss: 0.04467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19999/19999 [34:50<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "L1 Loss: 0.04125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 15225/19999 [26:24<08:18,  9.58it/s]"
     ]
    }
   ],
   "source": [
    "def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n",
    "    for e in range(epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        for data in tqdm(train_dl):\n",
    "            L, ab = data['L'].to(device), data['ab'].to(device)\n",
    "            preds = net_G(L)\n",
    "            loss = criterion(preds, ab)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            loss_meter.update(loss.item(), L.size(0))\n",
    "            \n",
    "        print(f\"Epoch {e + 1}/{epochs}\")\n",
    "        print(f\"L1 Loss: {loss_meter.avg:.5f}\")\n",
    "\n",
    "net_G = build_fastai_model(in_channels=1, out_channels=2, image_shape=(224,224))\n",
    "opt = torch.optim.Adam(net_G.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()        \n",
    "pretrain_generator(net_G, train_dl, opt, criterion, 20)\n",
    "#torch.save(net_G.state_dict(), \"res18-unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('woosuk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75d037d07eefe483d2728dfca4c3ea8c0758a49eeaf96553b1779a8462b7514f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
